{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparaci√≥n de Dataset para Clasificaci√≥n de Eventos S√≠smicos\n",
    "\n",
    "Este notebook prepara los datos para tareas de clasificaci√≥n:\n",
    "\n",
    "1. **Resampling:** Todas las se√±ales a 200 Hz\n",
    "2. **Estandarizaci√≥n de duraci√≥n:** Todos los eventos con la misma longitud\n",
    "3. **Padding:** Para eventos cortos (sin perder datos)\n",
    "4. **Formato listo:** Arrays numpy para modelos de ML/DL\n",
    "5. **Divisi√≥n:** Train/Validation/Test\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "import obspy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "with open(PROJECT_ROOT / \"config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "LABELED_DATA = Path(cfg[\"paths\"][\"labeled_data\"])\n",
    "FIGURES_DIR = PROJECT_ROOT / cfg[\"paths\"][\"figures_output\"]\n",
    "TABLES_DIR = PROJECT_ROOT / cfg[\"paths\"][\"tables_output\"]\n",
    "\n",
    "# Directorio para guardar dataset procesado\n",
    "DATASET_DIR = PROJECT_ROOT / \"processed_data\"\n",
    "DATASET_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚úì Configuraci√≥n cargada\")\n",
    "print(f\"Datos origen: {LABELED_DATA}\")\n",
    "print(f\"Dataset procesado: {DATASET_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploraci√≥n Inicial: Frecuencias y Duraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analizando archivos...\\n\")\n",
    "\n",
    "event_info = []\n",
    "\n",
    "for event_dir in LABELED_DATA.iterdir():\n",
    "    if not event_dir.is_dir():\n",
    "        continue\n",
    "    \n",
    "    event_type = event_dir.name\n",
    "    mseed_files = [f for f in event_dir.iterdir() if f.is_file()]\n",
    "    \n",
    "    print(f\"Procesando {event_type}... \", end=\"\", flush=True)\n",
    "    \n",
    "    for mseed_file in mseed_files:\n",
    "        try:\n",
    "            st = obspy.read(str(mseed_file), headonly=True)\n",
    "            tr = st[0]\n",
    "            \n",
    "            event_info.append({\n",
    "                'tipo': event_type,\n",
    "                'archivo': mseed_file.name,\n",
    "                'filepath': str(mseed_file),\n",
    "                'sampling_rate': tr.stats.sampling_rate,\n",
    "                'npts': tr.stats.npts,\n",
    "                'duracion_s': float(tr.stats.endtime - tr.stats.starttime)\n",
    "            })\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(f\"‚úì {len([e for e in event_info if e['tipo'] == event_type])} archivos\")\n",
    "\n",
    "df_info = pd.DataFrame(event_info)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RESUMEN DE DATOS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total de eventos: {len(df_info)}\")\n",
    "print(f\"\\nDistribuci√≥n por tipo:\")\n",
    "print(df_info['tipo'].value_counts())\n",
    "\n",
    "print(f\"\\nFrecuencias de muestreo encontradas:\")\n",
    "print(df_info['sampling_rate'].value_counts())\n",
    "\n",
    "print(f\"\\nEstad√≠sticas de duraci√≥n:\")\n",
    "print(df_info.groupby('tipo')['duracion_s'].agg(['min', 'max', 'mean', 'std']))\n",
    "\n",
    "# Encontrar duraci√≥n m√°xima\n",
    "max_duration = df_info['duracion_s'].max()\n",
    "max_event = df_info.loc[df_info['duracion_s'].idxmax()]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"DURACI√ìN M√ÅXIMA: {max_duration:.2f} segundos\")\n",
    "print(f\"Tipo: {max_event['tipo']}\")\n",
    "print(f\"Archivo: {max_event['archivo']}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualizaci√≥n de Distribuciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Duraci√≥n por tipo\n",
    "df_info.boxplot(column='duracion_s', by='tipo', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Duraci√≥n por Tipo de Evento', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Duraci√≥n (s)')\n",
    "axes[0, 0].get_figure().suptitle('')\n",
    "\n",
    "# Histograma de duraciones\n",
    "for event_type in df_info['tipo'].unique():\n",
    "    subset = df_info[df_info['tipo'] == event_type]\n",
    "    axes[0, 1].hist(subset['duracion_s'], alpha=0.6, label=event_type, bins=30)\n",
    "axes[0, 1].axvline(max_duration, color='red', linestyle='--', linewidth=2, label=f'Max: {max_duration:.1f}s')\n",
    "axes[0, 1].set_xlabel('Duraci√≥n (s)')\n",
    "axes[0, 1].set_ylabel('Frecuencia')\n",
    "axes[0, 1].set_title('Distribuci√≥n de Duraciones', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# N√∫mero de muestras por tipo\n",
    "df_info.boxplot(column='npts', by='tipo', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('N√∫mero de Muestras por Tipo', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('N√∫mero de muestras')\n",
    "axes[1, 0].get_figure().suptitle('')\n",
    "\n",
    "# Frecuencia de muestreo\n",
    "sampling_counts = df_info.groupby(['tipo', 'sampling_rate']).size().unstack(fill_value=0)\n",
    "sampling_counts.plot(kind='bar', ax=axes[1, 1], width=0.8)\n",
    "axes[1, 1].set_title('Frecuencias de Muestreo por Tipo', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Cantidad de eventos')\n",
    "axes[1, 1].set_xlabel('Tipo de Evento')\n",
    "axes[1, 1].legend(title='Sampling Rate (Hz)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"dataset_info_duraciones.png\", dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Gr√°fico guardado\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuraci√≥n del Procesamiento\n",
    "\n",
    "Definimos par√°metros para la estandarizaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par√°metros de procesamiento\n",
    "TARGET_SAMPLING_RATE = 200  # Hz\n",
    "TARGET_DURATION = np.ceil(max_duration)  # Redondear hacia arriba\n",
    "TARGET_SAMPLES = int(TARGET_SAMPLING_RATE * TARGET_DURATION)\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"PAR√ÅMETROS DE PROCESAMIENTO\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Frecuencia objetivo: {TARGET_SAMPLING_RATE} Hz\")\n",
    "print(f\"Duraci√≥n objetivo: {TARGET_DURATION} segundos\")\n",
    "print(f\"N√∫mero de muestras objetivo: {TARGET_SAMPLES}\")\n",
    "print(f\"\\nTodos los eventos ser√°n procesados a esta configuraci√≥n.\")\n",
    "print(f\"Eventos m√°s cortos: padding con ceros\")\n",
    "print(f\"Eventos m√°s largos: truncado (NO deber√≠a ocurrir)\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Funciones de Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_trace(trace, target_sampling_rate):\n",
    "    \"\"\"\n",
    "    Resamplea un trace a la frecuencia objetivo.\n",
    "    \n",
    "    Args:\n",
    "        trace: ObsPy Trace\n",
    "        target_sampling_rate: Frecuencia objetivo en Hz\n",
    "    \n",
    "    Returns:\n",
    "        ObsPy Trace resampleado\n",
    "    \"\"\"\n",
    "    current_rate = trace.stats.sampling_rate\n",
    "    \n",
    "    if current_rate == target_sampling_rate:\n",
    "        return trace.copy()\n",
    "    \n",
    "    # Calcular factor de resampling\n",
    "    if current_rate > target_sampling_rate:\n",
    "        # Decimaci√≥n\n",
    "        factor = int(current_rate / target_sampling_rate)\n",
    "        if current_rate / target_sampling_rate == factor:\n",
    "            # Factor entero, usar decimate\n",
    "            trace_copy = trace.copy()\n",
    "            trace_copy.decimate(factor=factor, no_filter=False)\n",
    "            return trace_copy\n",
    "        else:\n",
    "            # Factor no entero, usar resample\n",
    "            trace_copy = trace.copy()\n",
    "            trace_copy.resample(sampling_rate=target_sampling_rate)\n",
    "            return trace_copy\n",
    "    else:\n",
    "        # Interpolaci√≥n\n",
    "        trace_copy = trace.copy()\n",
    "        trace_copy.resample(sampling_rate=target_sampling_rate)\n",
    "        return trace_copy\n",
    "\n",
    "\n",
    "def standardize_length(data, target_length, padding_mode='constant'):\n",
    "    \"\"\"\n",
    "    Estandariza la longitud de un array.\n",
    "    \n",
    "    Args:\n",
    "        data: Array numpy\n",
    "        target_length: Longitud objetivo\n",
    "        padding_mode: 'constant' (ceros) o 'edge' (repetir √∫ltimo valor)\n",
    "    \n",
    "    Returns:\n",
    "        Array de longitud target_length\n",
    "    \"\"\"\n",
    "    current_length = len(data)\n",
    "    \n",
    "    if current_length == target_length:\n",
    "        return data\n",
    "    elif current_length < target_length:\n",
    "        # Padding\n",
    "        pad_width = target_length - current_length\n",
    "        if padding_mode == 'constant':\n",
    "            return np.pad(data, (0, pad_width), mode='constant', constant_values=0)\n",
    "        elif padding_mode == 'edge':\n",
    "            return np.pad(data, (0, pad_width), mode='edge')\n",
    "    else:\n",
    "        # Truncar (no deber√≠a pasar si TARGET_DURATION es el m√°ximo)\n",
    "        return data[:target_length]\n",
    "\n",
    "\n",
    "def process_event(filepath, target_sampling_rate, target_samples):\n",
    "    \"\"\"\n",
    "    Procesa un evento completo: resample + estandarizaci√≥n.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Ruta al archivo MSeed\n",
    "        target_sampling_rate: Frecuencia objetivo\n",
    "        target_samples: N√∫mero de muestras objetivo\n",
    "    \n",
    "    Returns:\n",
    "        numpy array de forma (target_samples,) o None si hay error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Leer archivo\n",
    "        st = obspy.read(filepath)\n",
    "        tr = st[0]  # Primer trace\n",
    "        \n",
    "        # Resample\n",
    "        tr_resampled = resample_trace(tr, target_sampling_rate)\n",
    "        \n",
    "        # Estandarizar longitud\n",
    "        data = tr_resampled.data.astype(np.float32)\n",
    "        data_standardized = standardize_length(data, target_samples, padding_mode='constant')\n",
    "        \n",
    "        return data_standardized\n",
    "    \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"‚úì Funciones de procesamiento definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Procesamiento de Todos los Eventos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Procesando eventos...\\n\")\n",
    "\n",
    "X_data = []  # Se√±ales\n",
    "y_labels = []  # Etiquetas (tipo de evento)\n",
    "metadata = []  # Informaci√≥n adicional\n",
    "\n",
    "for idx, row in tqdm(df_info.iterrows(), total=len(df_info), desc=\"Procesando\"):\n",
    "    # Procesar evento\n",
    "    signal_data = process_event(\n",
    "        filepath=row['filepath'],\n",
    "        target_sampling_rate=TARGET_SAMPLING_RATE,\n",
    "        target_samples=TARGET_SAMPLES\n",
    "    )\n",
    "    \n",
    "    if signal_data is not None:\n",
    "        X_data.append(signal_data)\n",
    "        y_labels.append(row['tipo'])\n",
    "        \n",
    "        metadata.append({\n",
    "            'tipo': row['tipo'],\n",
    "            'archivo': row['archivo'],\n",
    "            'duracion_original': row['duracion_s'],\n",
    "            'sampling_rate_original': row['sampling_rate']\n",
    "        })\n",
    "\n",
    "# Convertir a arrays numpy\n",
    "X = np.array(X_data, dtype=np.float32)\n",
    "y_str = np.array(y_labels)\n",
    "\n",
    "# Encodear labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_str)\n",
    "\n",
    "# Metadata\n",
    "metadata_df = pd.DataFrame(metadata)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATASET PROCESADO\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Forma de X: {X.shape}\")\n",
    "print(f\"  - {X.shape[0]} eventos\")\n",
    "print(f\"  - {X.shape[1]} muestras por evento\")\n",
    "print(f\"  - Duraci√≥n: {X.shape[1] / TARGET_SAMPLING_RATE:.2f} segundos\")\n",
    "print(f\"\\nForma de y: {y.shape}\")\n",
    "print(f\"\\nClases:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    count = np.sum(y == i)\n",
    "    print(f\"  {i}: {class_name} ({count} eventos, {count/len(y)*100:.1f}%)\")\n",
    "print(f\"\\nTama√±o en memoria: {X.nbytes / (1024**2):.2f} MB\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Normalizaci√≥n de Se√±ales\n",
    "\n",
    "Normalizamos cada se√±al individualmente (z-score normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_signals(X, method='zscore'):\n",
    "    \"\"\"\n",
    "    Normaliza se√±ales.\n",
    "    \n",
    "    Args:\n",
    "        X: Array (n_samples, n_timesteps)\n",
    "        method: 'zscore', 'minmax', o 'robust'\n",
    "    \n",
    "    Returns:\n",
    "        X normalizado\n",
    "    \"\"\"\n",
    "    X_norm = X.copy()\n",
    "    \n",
    "    if method == 'zscore':\n",
    "        # Z-score por se√±al\n",
    "        mean = X_norm.mean(axis=1, keepdims=True)\n",
    "        std = X_norm.std(axis=1, keepdims=True) + 1e-8  # Evitar divisi√≥n por cero\n",
    "        X_norm = (X_norm - mean) / std\n",
    "    \n",
    "    elif method == 'minmax':\n",
    "        # Min-Max por se√±al\n",
    "        min_val = X_norm.min(axis=1, keepdims=True)\n",
    "        max_val = X_norm.max(axis=1, keepdims=True)\n",
    "        X_norm = (X_norm - min_val) / (max_val - min_val + 1e-8)\n",
    "    \n",
    "    elif method == 'robust':\n",
    "        # Robust scaling (mediana e IQR)\n",
    "        median = np.median(X_norm, axis=1, keepdims=True)\n",
    "        q75 = np.percentile(X_norm, 75, axis=1, keepdims=True)\n",
    "        q25 = np.percentile(X_norm, 25, axis=1, keepdims=True)\n",
    "        iqr = q75 - q25 + 1e-8\n",
    "        X_norm = (X_norm - median) / iqr\n",
    "    \n",
    "    return X_norm\n",
    "\n",
    "\n",
    "# Normalizar\n",
    "X_normalized = normalize_signals(X, method='zscore')\n",
    "\n",
    "print(\"‚úì Se√±ales normalizadas (z-score)\")\n",
    "print(f\"  Media: {X_normalized.mean():.6f}\")\n",
    "print(f\"  Std: {X_normalized.std():.6f}\")\n",
    "print(f\"  Min: {X_normalized.min():.2f}\")\n",
    "print(f\"  Max: {X_normalized.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Divisi√≥n Train/Validation/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisi√≥n estratificada\n",
    "# 70% train, 15% val, 15% test\n",
    "\n",
    "# Primero: train+val (85%) vs test (15%)\n",
    "X_temp, X_test, y_temp, y_test, meta_temp, meta_test = train_test_split(\n",
    "    X_normalized, y, metadata_df,\n",
    "    test_size=0.15,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Luego: train (70%) vs val (15%)\n",
    "# 15% de 85% = 17.65% del temp\n",
    "X_train, X_val, y_train, y_val, meta_train, meta_val = train_test_split(\n",
    "    X_temp, y_temp, meta_temp,\n",
    "    test_size=0.1765,\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"DIVISI√ìN DEL DATASET\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nTrain:\")\n",
    "print(f\"  Shape: {X_train.shape}\")\n",
    "print(f\"  Porcentaje: {len(X_train)/len(X)*100:.1f}%\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    count = np.sum(y_train == i)\n",
    "    print(f\"    {class_name}: {count}\")\n",
    "\n",
    "print(f\"\\nValidation:\")\n",
    "print(f\"  Shape: {X_val.shape}\")\n",
    "print(f\"  Porcentaje: {len(X_val)/len(X)*100:.1f}%\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    count = np.sum(y_val == i)\n",
    "    print(f\"    {class_name}: {count}\")\n",
    "\n",
    "print(f\"\\nTest:\")\n",
    "print(f\"  Shape: {X_test.shape}\")\n",
    "print(f\"  Porcentaje: {len(X_test)/len(X)*100:.1f}%\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    count = np.sum(y_test == i)\n",
    "    print(f\"    {class_name}: {count}\")\n",
    "\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizaci√≥n de Ejemplos Procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar un ejemplo aleatorio de cada clase\n",
    "fig, axes = plt.subplots(len(label_encoder.classes_), 2, figsize=(16, 3*len(label_encoder.classes_)))\n",
    "\n",
    "fig.suptitle('Ejemplos de Se√±ales Procesadas por Tipo', fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    # Encontrar √≠ndice de un ejemplo de esta clase\n",
    "    idx = np.where(y_train == i)[0][0]\n",
    "    \n",
    "    signal_data = X_train[idx]\n",
    "    time = np.arange(len(signal_data)) / TARGET_SAMPLING_RATE\n",
    "    \n",
    "    # Dominio del tiempo\n",
    "    axes[i, 0].plot(time, signal_data, linewidth=0.5)\n",
    "    axes[i, 0].set_title(f'{class_name} - Dominio del Tiempo', fontsize=11, fontweight='bold')\n",
    "    axes[i, 0].set_xlabel('Tiempo (s)')\n",
    "    axes[i, 0].set_ylabel('Amplitud (normalizada)')\n",
    "    axes[i, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Espectrograma\n",
    "    f, t, Sxx = signal.spectrogram(signal_data, TARGET_SAMPLING_RATE, nperseg=256)\n",
    "    im = axes[i, 1].pcolormesh(t, f, 10 * np.log10(Sxx + 1e-10), shading='gouraud', cmap='viridis')\n",
    "    axes[i, 1].set_title(f'{class_name} - Espectrograma', fontsize=11, fontweight='bold')\n",
    "    axes[i, 1].set_xlabel('Tiempo (s)')\n",
    "    axes[i, 1].set_ylabel('Frecuencia (Hz)')\n",
    "    axes[i, 1].set_ylim(0, 50)\n",
    "    plt.colorbar(im, ax=axes[i, 1], label='Potencia (dB)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"ejemplos_senales_procesadas.png\", dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Gr√°fico guardado\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Guardar Dataset Procesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Guardar en formato .npz (comprimido)\n",
    "np.savez_compressed(\n",
    "    DATASET_DIR / 'seismic_classification_dataset.npz',\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    sampling_rate=TARGET_SAMPLING_RATE,\n",
    "    duration=TARGET_DURATION,\n",
    "    n_samples=TARGET_SAMPLES\n",
    ")\n",
    "\n",
    "# Guardar label encoder\n",
    "with open(DATASET_DIR / 'label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# Guardar metadata\n",
    "meta_train.to_csv(DATASET_DIR / 'metadata_train.csv', index=False)\n",
    "meta_val.to_csv(DATASET_DIR / 'metadata_val.csv', index=False)\n",
    "meta_test.to_csv(DATASET_DIR / 'metadata_test.csv', index=False)\n",
    "\n",
    "# Guardar informaci√≥n del dataset\n",
    "dataset_info = {\n",
    "    'n_classes': len(label_encoder.classes_),\n",
    "    'classes': label_encoder.classes_.tolist(),\n",
    "    'n_train': len(X_train),\n",
    "    'n_val': len(X_val),\n",
    "    'n_test': len(X_test),\n",
    "    'sampling_rate': TARGET_SAMPLING_RATE,\n",
    "    'duration_seconds': float(TARGET_DURATION),\n",
    "    'n_samples': TARGET_SAMPLES,\n",
    "    'normalization': 'zscore',\n",
    "    'train_split': 0.70,\n",
    "    'val_split': 0.15,\n",
    "    'test_split': 0.15\n",
    "}\n",
    "\n",
    "with open(DATASET_DIR / 'dataset_info.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset_info, f)\n",
    "\n",
    "# Tambi√©n en JSON para lectura f√°cil\n",
    "import json\n",
    "with open(DATASET_DIR / 'dataset_info.json', 'w') as f:\n",
    "    json.dump(dataset_info, f, indent=2)\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"ARCHIVOS GUARDADOS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nDirectorio: {DATASET_DIR}\")\n",
    "print(f\"\\nArchivos:\")\n",
    "print(f\"  1. seismic_classification_dataset.npz (datos principales)\")\n",
    "print(f\"  2. label_encoder.pkl (encoder de clases)\")\n",
    "print(f\"  3. metadata_train/val/test.csv (metadatos)\")\n",
    "print(f\"  4. dataset_info.json (informaci√≥n del dataset)\")\n",
    "\n",
    "# Calcular tama√±o\n",
    "total_size = sum((DATASET_DIR / f).stat().st_size \n",
    "                 for f in ['seismic_classification_dataset.npz', \n",
    "                          'label_encoder.pkl',\n",
    "                          'metadata_train.csv',\n",
    "                          'metadata_val.csv',\n",
    "                          'metadata_test.csv'])\n",
    "\n",
    "print(f\"\\nTama√±o total: {total_size / (1024**2):.2f} MB\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. C√≥digo para Cargar el Dataset\n",
    "\n",
    "Ejemplo de c√≥mo cargar los datos procesados en otro notebook o script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√ìDIGO PARA CARGAR EL DATASET PROCESADO\n",
    "# Copia esto en tu notebook de entrenamiento\n",
    "\n",
    "load_example = '''\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Ruta al dataset\n",
    "DATASET_DIR = Path(\"processed_data\")\n",
    "\n",
    "# Cargar datos\n",
    "data = np.load(DATASET_DIR / \"seismic_classification_dataset.npz\")\n",
    "\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_val = data['X_val']\n",
    "y_val = data['y_val']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "\n",
    "# Cargar label encoder\n",
    "with open(DATASET_DIR / 'label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# Cargar info del dataset\n",
    "with open(DATASET_DIR / 'dataset_info.json', 'r') as f:\n",
    "    dataset_info = json.load(f)\n",
    "\n",
    "print(f\"Dataset cargado:\")\n",
    "print(f\"  Train: {X_train.shape}\")\n",
    "print(f\"  Val: {X_val.shape}\")\n",
    "print(f\"  Test: {X_test.shape}\")\n",
    "print(f\"  Clases: {dataset_info['classes']}\")\n",
    "\n",
    "# Para TensorFlow/Keras, agregar dimensi√≥n de canal\n",
    "X_train_tf = X_train[..., np.newaxis]  # Shape: (n, timesteps, 1)\n",
    "X_val_tf = X_val[..., np.newaxis]\n",
    "X_test_tf = X_test[..., np.newaxis]\n",
    "\n",
    "# Para PyTorch, cambiar orden de dimensiones\n",
    "import torch\n",
    "X_train_torch = torch.from_numpy(X_train).unsqueeze(1)  # Shape: (n, 1, timesteps)\n",
    "y_train_torch = torch.from_numpy(y_train).long()\n",
    "'''\n",
    "\n",
    "print(\"EJEMPLO DE C√ìDIGO PARA CARGAR EL DATASET:\")\n",
    "print(\"=\"*60)\n",
    "print(load_example)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Resumen Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úì PROCESAMIENTO COMPLETADO\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nüìä DATASET FINAL:\")\n",
    "print(f\"  ‚Ä¢ {len(X)} eventos procesados\")\n",
    "print(f\"  ‚Ä¢ {len(label_encoder.classes_)} clases: {', '.join(label_encoder.classes_)}\")\n",
    "print(f\"  ‚Ä¢ Frecuencia: {TARGET_SAMPLING_RATE} Hz\")\n",
    "print(f\"  ‚Ä¢ Duraci√≥n: {TARGET_DURATION} segundos\")\n",
    "print(f\"  ‚Ä¢ Muestras por evento: {TARGET_SAMPLES}\")\n",
    "\n",
    "print(f\"\\nüìÇ DIVISI√ìN:\")\n",
    "print(f\"  ‚Ä¢ Train: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"  ‚Ä¢ Validation: {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"  ‚Ä¢ Test: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ CARACTER√çSTICAS:\")\n",
    "print(f\"  ‚Ä¢ Todas las se√±ales a 200 Hz\")\n",
    "print(f\"  ‚Ä¢ Duraci√≥n estandarizada (padding con ceros)\")\n",
    "print(f\"  ‚Ä¢ Normalizaci√≥n z-score por se√±al\")\n",
    "print(f\"  ‚Ä¢ Divisi√≥n estratificada\")\n",
    "\n",
    "print(f\"\\nüíæ ARCHIVOS GUARDADOS EN: {DATASET_DIR}\")\n",
    "\n",
    "print(f\"\\nüöÄ PR√ìXIMOS PASOS:\")\n",
    "print(f\"  1. Entrenar modelos baseline (RF, XGBoost)\")\n",
    "print(f\"  2. Desarrollar modelos CNN-LSTM\")\n",
    "print(f\"  3. Evaluar con X_test y y_test\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
