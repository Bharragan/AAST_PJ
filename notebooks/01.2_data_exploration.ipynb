{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f502f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT : c:\\Users\\Disc\\Desktop\\pj\n",
      "FIGURES_DIR  : C:\\Users\\Disc\\Desktop\\pj\\figures\n",
      "TABLES_DIR   : C:\\Users\\Disc\\Desktop\\pj\\tables\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "import obspy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "with open(PROJECT_ROOT / \"config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "DATA_ROOT = Path(cfg[\"paths\"][\"data_root\"])\n",
    "LABELED_DATA = Path(cfg[\"paths\"][\"labeled_data\"])\n",
    "UNLABELED_DATA = Path(cfg[\"paths\"][\"unlabeled_data\"])\n",
    "\n",
    "FIGURES_DIR = PROJECT_ROOT / cfg[\"paths\"][\"figures_output\"]\n",
    "TABLES_DIR = PROJECT_ROOT / cfg[\"paths\"][\"tables_output\"]\n",
    "NOTEBOOKS_DIR = PROJECT_ROOT / \"notebooks\"\n",
    "\n",
    "FIGURES_DIR.mkdir(exist_ok=True)\n",
    "TABLES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT :\", PROJECT_ROOT)\n",
    "print(\"FIGURES_DIR  :\", FIGURES_DIR.resolve())\n",
    "print(\"TABLES_DIR   :\", TABLES_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80cedc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos en UNLABELED_DATA: 13761\n"
     ]
    }
   ],
   "source": [
    "n_files = sum(1 for p in UNLABELED_DATA.rglob(\"*\") if p.is_file())\n",
    "print(f\"Archivos en UNLABELED_DATA: {n_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81848bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  network station location channel  year  doy                 filename\n",
      "0      CL    LC03       01     HHZ  2021  141  CL.LC03.01.HHZ.2021.141\n",
      "1      CL    LC03       01     HHZ  2021  151  CL.LC03.01.HHZ.2021.151\n",
      "2      CL    LC03       01     HHN  2021  141  CL.LC03.01.HHN.2021.141\n",
      "3      CL    LC03       01     HHN  2021  151  CL.LC03.01.HHN.2021.151\n",
      "4      CL    LC03       01     HHE  2021  141  CL.LC03.01.HHE.2021.141\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "files = [p.name for p in UNLABELED_DATA.rglob(\"*\") if p.is_file()]\n",
    "\n",
    "records = []\n",
    "for f in files:\n",
    "    parts = f.split(\".\")\n",
    "    if len(parts) == 6:\n",
    "        net, sta, loc, chan, year, doy = parts\n",
    "        records.append({\n",
    "            \"network\": net,\n",
    "            \"station\": sta,\n",
    "            \"location\": loc,\n",
    "            \"channel\": chan,\n",
    "            \"year\": int(year),\n",
    "            \"doy\": int(doy),\n",
    "            \"filename\": f\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0feae1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estaciones:\n",
      " - LC01\n",
      " - LC02\n",
      " - LC03\n",
      " - LC04\n",
      " - LC05\n",
      " - LC06\n",
      " - LC07\n",
      " - LC08\n",
      " - LC09\n",
      " - LC10\n",
      " - LC11\n"
     ]
    }
   ],
   "source": [
    "stations = sorted(df[\"station\"].unique())\n",
    "print(\"Estaciones:\")\n",
    "for s in stations:\n",
    "    print(\" -\", s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d58d6a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station\n",
      "LC03    3321\n",
      "LC01    3180\n",
      "LC02    2661\n",
      "LC04    1914\n",
      "LC06     450\n",
      "LC08     450\n",
      "LC09     447\n",
      "LC10     447\n",
      "LC07     429\n",
      "LC11     279\n",
      "LC05     183\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "events_per_station = df.groupby(\"station\").size().sort_values(ascending=False)\n",
    "print(events_per_station)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e9c29fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel\n",
      "HHE    4587\n",
      "HHN    4587\n",
      "HHZ    4587\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "events_per_channel = df.groupby(\"channel\").size()\n",
    "print(events_per_channel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "095eb587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel   HHE   HHN   HHZ\n",
      "station                  \n",
      "LC01     1060  1060  1060\n",
      "LC02      887   887   887\n",
      "LC03     1107  1107  1107\n",
      "LC04      638   638   638\n",
      "LC05       61    61    61\n",
      "LC06      150   150   150\n",
      "LC07      143   143   143\n",
      "LC08      150   150   150\n",
      "LC09      149   149   149\n",
      "LC10      149   149   149\n",
      "LC11       93    93    93\n"
     ]
    }
   ],
   "source": [
    "station_channel = df.groupby([\"station\", \"channel\"]).size().unstack(fill_value=0)\n",
    "print(station_channel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19d03563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year\n",
      "2018    4200\n",
      "2021    1956\n",
      "2022    3447\n",
      "2023    3363\n",
      "2024     795\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "events_per_year = df.groupby(\"year\").size().sort_index()\n",
    "print(events_per_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcb3ee89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station  LC01  LC02  LC03  LC04  LC05  LC06  LC07  LC08  LC09  LC10  LC11\n",
      "year                                                                     \n",
      "2018      333     6   483   693   183   450   429   450   447   447   279\n",
      "2021      654   654   648     0     0     0     0     0     0     0     0\n",
      "2022      948   945   945   609     0     0     0     0     0     0     0\n",
      "2023     1095   810  1095   363     0     0     0     0     0     0     0\n",
      "2024      150   246   150   249     0     0     0     0     0     0     0\n"
     ]
    }
   ],
   "source": [
    "events_station_year = (\n",
    "    df.groupby([\"year\", \"station\"])\n",
    "      .size()\n",
    "      .unstack(fill_value=0)\n",
    "      .sort_index()\n",
    ")\n",
    "\n",
    "print(events_station_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ba640f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel       HHE  HHN  HHZ\n",
      "year station               \n",
      "2018 LC01     111  111  111\n",
      "     LC02       2    2    2\n",
      "     LC03     161  161  161\n",
      "     LC04     231  231  231\n",
      "     LC05      61   61   61\n",
      "     LC06     150  150  150\n",
      "     LC07     143  143  143\n",
      "     LC08     150  150  150\n",
      "     LC09     149  149  149\n",
      "     LC10     149  149  149\n",
      "     LC11      93   93   93\n",
      "2021 LC01     218  218  218\n",
      "     LC02     218  218  218\n",
      "     LC03     216  216  216\n",
      "2022 LC01     316  316  316\n",
      "     LC02     315  315  315\n",
      "     LC03     315  315  315\n",
      "     LC04     203  203  203\n",
      "2023 LC01     365  365  365\n",
      "     LC02     270  270  270\n",
      "     LC03     365  365  365\n",
      "     LC04     121  121  121\n",
      "2024 LC01      50   50   50\n",
      "     LC02      82   82   82\n",
      "     LC03      50   50   50\n",
      "     LC04      83   83   83\n"
     ]
    }
   ],
   "source": [
    "events_full = (\n",
    "    df[df[\"channel\"].isin([\"HHE\", \"HHN\", \"HHZ\"])]\n",
    "    .groupby([\"year\", \"station\", \"channel\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "print(events_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5071f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year  channel\n",
      "2022  HHE        316\n",
      "      HHN        316\n",
      "      HHZ        316\n",
      "2023  HHE        365\n",
      "      HHN        365\n",
      "      HHZ        365\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "subset_df = df[\n",
    "    (df[\"station\"] == \"LC01\") &\n",
    "    (df[\"year\"].isin([2022, 2023])) &\n",
    "    (df[\"channel\"].isin([\"HHE\", \"HHN\", \"HHZ\"]))\n",
    "].copy()\n",
    "\n",
    "print(subset_df.groupby([\"year\", \"channel\"]).size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbdb2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos a procesar: 316\n",
      "Escribiendo en: seismic_LC01_HHZ_2022.csv\n",
      "Modo: Escritura incremental (baja RAM)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando:   2%|▏         | 7/316 [00:45<32:16,  6.27s/it]"
     ]
    }
   ],
   "source": [
    "# VERSIÓN OPTIMIZADA - Copia esto directamente en tu notebook\n",
    "# Usa MÍNIMA RAM escribiendo incrementalmente a disco\n",
    "\n",
    "from obspy import read\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# Configuración\n",
    "year = 2022\n",
    "station = \"LC01\"\n",
    "channel = \"HHZ\"\n",
    "output_file = f\"seismic_{station}_{channel}_{year}.csv\"\n",
    "\n",
    "# Filtrar archivos\n",
    "files_2022 = subset_df[\n",
    "    (subset_df[\"year\"] == year) &\n",
    "    (subset_df[\"station\"] == station) &\n",
    "    (subset_df[\"channel\"] == channel)\n",
    "].sort_values(\"doy\")\n",
    "\n",
    "print(f\"Archivos a procesar: {len(files_2022)}\")\n",
    "print(f\"Escribiendo en: {output_file}\")\n",
    "print(\"Modo: Escritura incremental (baja RAM)\\n\")\n",
    "\n",
    "# Variables de control\n",
    "total_samples = 0\n",
    "files_processed = 0\n",
    "chunk_size = 100  # Escribir cada N archivos\n",
    "buffer = []\n",
    "\n",
    "# Crear archivo (vacío inicial)\n",
    "write_header = True\n",
    "mode = 'w'\n",
    "\n",
    "for idx, row in tqdm(files_2022.iterrows(), total=len(files_2022), desc=\"Procesando\"):\n",
    "    file_path = UNLABELED_DATA / row[\"filename\"]\n",
    "    \n",
    "    try:\n",
    "        # Leer y decimar\n",
    "        st = read(file_path)\n",
    "        st.decimate(factor=10, no_filter=False)  # 200 Hz -> 20 Hz\n",
    "        \n",
    "        for tr in st:\n",
    "            # Calcular timestamps de forma vectorizada (MUCHO más rápido)\n",
    "            start_time = tr.stats.starttime\n",
    "            sampling_rate = tr.stats.sampling_rate\n",
    "            n_samples = tr.stats.npts\n",
    "            \n",
    "            # Array de tiempos\n",
    "            time_offsets = np.arange(n_samples) / sampling_rate\n",
    "            timestamps = [start_time + offset for offset in time_offsets]\n",
    "            timestamps_dt = [t.datetime for t in timestamps]\n",
    "            \n",
    "            # DataFrame del trace\n",
    "            df_trace = pd.DataFrame({\n",
    "                'datetime': timestamps_dt,\n",
    "                'value': tr.data.astype(np.float32)  # float32 ahorra 50% de memoria\n",
    "            })\n",
    "            \n",
    "            buffer.append(df_trace)\n",
    "            total_samples += len(df_trace)\n",
    "        \n",
    "        files_processed += 1\n",
    "        \n",
    "        # Escribir a disco cada chunk_size archivos\n",
    "        if files_processed % chunk_size == 0:\n",
    "            if buffer:\n",
    "                # Concatenar buffer\n",
    "                df_chunk = pd.concat(buffer, ignore_index=True)\n",
    "                \n",
    "                # Escribir a CSV\n",
    "                df_chunk.to_csv(\n",
    "                    output_file,\n",
    "                    mode=mode,\n",
    "                    header=write_header,\n",
    "                    index=False,\n",
    "                    date_format='%Y-%m-%d %H:%M:%S.%f'\n",
    "                )\n",
    "                \n",
    "                # Limpiar memoria\n",
    "                buffer.clear()\n",
    "                del df_chunk\n",
    "                gc.collect()\n",
    "                \n",
    "                # Después del primer write, usar append mode\n",
    "                mode = 'a'\n",
    "                write_header = False\n",
    "                \n",
    "                print(f\"✓ Escritos {total_samples:,} samples ({files_processed}/{len(files_2022)} archivos)\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error en {row['filename']}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Escribir último chunk\n",
    "if buffer:\n",
    "    df_chunk = pd.concat(buffer, ignore_index=True)\n",
    "    df_chunk.to_csv(\n",
    "        output_file,\n",
    "        mode=mode,\n",
    "        header=write_header,\n",
    "        index=False,\n",
    "        date_format='%Y-%m-%d %H:%M:%S.%f'\n",
    "    )\n",
    "    del df_chunk\n",
    "    buffer.clear()\n",
    "    gc.collect()\n",
    "\n",
    "# Resumen final\n",
    "import os\n",
    "file_size_mb = os.path.getsize(output_file) / (1024**2)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ CONVERSIÓN COMPLETADA\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Archivos procesados: {files_processed}\")\n",
    "print(f\"  Muestras totales: {total_samples:,}\")\n",
    "print(f\"  Archivo: {output_file}\")\n",
    "print(f\"  Tamaño: {file_size_mb:.2f} MB\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AST",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
